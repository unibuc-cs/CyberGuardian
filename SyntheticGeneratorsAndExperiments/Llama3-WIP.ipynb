{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ce4203ee6697f8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T12:29:49.240480Z",
     "start_time": "2024-06-20T12:28:30.173366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262d6d69ab584489917857883b91926e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare model, tokenizer: 9.809 sec.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load a Llama-3-8B instruct \n",
    "import transformers\n",
    "import sys\n",
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from time import time\n",
    "#import chromadb\n",
    "#from chromadb.config import Settings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "device = \"auto\" # f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "time_start = time()\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "   model_id,\n",
    "    trust_remote_code=True,\n",
    "    #max_new_tokens=1024\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "time_end = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")\n",
    "\n",
    "def ask_model(messages):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)\n",
    "                            \n",
    "                   \n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a robot.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a song for Romania\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d340c7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciprian/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:533: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ciprian/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:538: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the output in JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"people\": [\n",
      "    {\n",
      "      \"name\": \"Ciprian\",\n",
      "      \"height_in_meters\": 1.8\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "This output conforms to the given JSON schema.\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import List, Optional\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The name of the person\")\n",
    "    height_in_meters: float = Field(\n",
    "        ..., description=\"The height of the person expressed in meters.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: List[Person]\n",
    "    \n",
    "    \n",
    "# Set up a parser\n",
    "parser = PydanticOutputParser(pydantic_object=People)\n",
    "\n",
    "# Get the format instructions in natural language\n",
    "parser_format_instr = parser.get_format_instructions() \n",
    "#print(f\"Parser format instructions: {parser_format_instr}\")\n",
    "\n",
    "# Setup the input template\n",
    "messages = [\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"{query}\"}\n",
    "    ]\n",
    "input_txt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False)\n",
    "\n",
    "# Langachain prompt template, partially filled with the format instructions above\n",
    "qa_prompt_template = PromptTemplate(\n",
    "    template=input_txt,\n",
    "    input_variables=[\"query\", \"format_instructions\"],\n",
    ").partial(format_instructions=parser_format_instr)\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "# define the pipeline\n",
    "qa_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, do_sample=False, temperature=0.0)#, max_new_tokens=10)\n",
    "hf = HuggingFacePipeline(pipeline=qa_pipe)\n",
    "\n",
    "# A function to respond on a query based on the prompt and model \n",
    "def ask_question_v2(query):\n",
    "    qa_formatted_question = qa_prompt_template.format_prompt(query=query).to_string()\n",
    "    len_of_formatted_question = len(qa_formatted_question)\n",
    "    chain = qa_prompt_template | hf.bind(skip_prompt=True) #| parser\n",
    "    raw_res = chain.invoke({'query':query})\n",
    "    res_without_format = raw_res[len_of_formatted_question:]\n",
    "    return res_without_format\n",
    "\n",
    "\n",
    "query = \"Ciprian is 37 and he is 1.8 meters tall\" \n",
    "answer = ask_question_v2(query)\n",
    "print(answer)\n",
    "\n",
    "\n",
    "\n",
    "# input_ids = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54f6a2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[Person(name='Ciprian', height_in_meters=1.8)]\n"
     ]
    }
   ],
   "source": [
    "print(parser.parse(answer))\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "import re \n",
    "import json\n",
    "\n",
    "# Custom parser\n",
    "def extract_json(message: AIMessage) -> List[dict]:\n",
    "    \"\"\"Extracts JSON content from a string where JSON is embedded between ```json and ``` tags.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text containing the JSON content.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted JSON strings.\n",
    "    \"\"\"\n",
    "    text = message.content\n",
    "    # Define the regular expression pattern to match JSON blocks\n",
    "    pattern = r\"```json(.*?)```\"\n",
    "\n",
    "    # Find all non-overlapping matches of the pattern in the string\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    # Return the list of matched JSON strings, stripping any leading or trailing whitespace\n",
    "    try:\n",
    "        return [json.loads(match.strip()) for match in matches]\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Failed to parse: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b53b8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_ai= AIMessage(content=answer)\n",
    "extract_json(answer_ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4cf3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
