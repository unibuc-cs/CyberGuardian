{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b7eef9-23a4-430a-bc89-d3002293e276",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-01-30T09:11:25.452075800Z",
     "start_time": "2024-01-30T09:11:19.341802100Z"
    }
   },
   "source": [
    "import secrets\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import pdb\n",
    "from typing import Any\n",
    "import random \n",
    "\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from etl import markdown, pdfs, shared, videos\n",
    "\n",
    "import docstore\n",
    "import vecstore\n",
    "from utils import pretty_log\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline, TextStreamer\n",
    "import json\n",
    "import textwrap\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import langchain\n",
    "import time\n",
    "\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from QuestionAndAnswerUtils import *"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b544796-7749-4ab5-888d-476eb2feb652",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-01-30T09:12:28.108098700Z",
     "start_time": "2024-01-30T09:11:34.394519700Z"
    }
   },
   "source": [
    "securityChatbot = QuestionAndAnsweringCustomLlama3(QuestionRewritingPrompt=QuestionAndAnsweringCustomLlama2.QUESTION_REWRITING_TYPE.QUESTION_REWRITING_DEFAULT,\n",
    "                                 QuestionAnsweringPrompt=QuestionAndAnsweringCustomLlama3.SECURITY_PROMPT_TYPE.PROMPT_TYPE_SECURITY_OFFICER_WITH_RAG_MEMORY_NOSOURCES,\n",
    "                                 ModelType=QuestionAndAnsweringCustomLlama3.LLAMA3_VERSION_TYPE.LLAMA3_13B,\n",
    "                                debug=True, streamingOnAnotherThread=False, demoMode=True, noInitialize=True)\n",
    "\n",
    "securityChatbot.initializePromptTemplates()\n",
    "securityChatbot.initializeLLMTokenizerandEmbedder()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pipe = pipeline(\"text-generation\",\n",
    "                                               model=securityChatbot.model,\n",
    "                                               tokenizer=securityChatbot.tokenizer,\n",
    "                                               torch_dtype=torch.float16,\n",
    "                                               device_map=\"auto\",\n",
    "                                               max_new_tokens=512,\n",
    "                                               do_sample=True,\n",
    "                                               temperature=0.9,\n",
    "                                                top_p=0.95,\n",
    "                                               min_length=None,\n",
    "                                               num_return_sequences=1,\n",
    "                                               repetition_penalty=1.0,\n",
    "                                               # The parameter for repetition penalty. 1.0 means no penalty.\n",
    "                                               #length_penalty=1,\n",
    "                                               # [optional] Exponential penalty to the length that is used with beam-based generation.\n",
    "                                               eos_token_id=securityChatbot.tokenizer.eos_token_id,\n",
    "                                               pad_token_id=securityChatbot.tokenizer.eos_token_id,\n",
    "                                               streamer=securityChatbot.streamer,\n",
    "                                               )\n",
    "\n",
    "# Create the llm here\n",
    "llm_custom = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "def create_prompt_with_rag():\n",
    "    qwithrag_template = get_prompt(\"Given the following information: {retrieved_info}, respond to: {question}\")\n",
    "    qwithrag_prompt = PromptTemplate(template=qwithrag_template,\n",
    "                                    input_variables=[\"retrieved_info\", \"question\"])\n",
    "    return qwithrag_prompt\n",
    "    \n",
    "responseCheckPrompt = create_prompt_with_rag()\n",
    "\n",
    "responseCheckChain = LLMChain(prompt=responseCheckPrompt, llm=llm_custom)\n",
    "\n",
    "\n",
    "test_question_with_rag(chain_instance=responseCheckChain)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:48:32.242225900Z",
     "start_time": "2024-01-30T09:48:21.324961300Z"
    }
   },
   "id": "37ec43a6805aa2a8",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "    import re\n",
    "from statistics import mode\n",
    "\n",
    "def gen_answer():\n",
    "    response = completion(\n",
    "        \"John found that the average of 15 numbers is 40.\"\n",
    "        \"If 10 is added to each number then the mean of the numbers is?\"\n",
    "        \"Report the answer surrounded by three backticks, for example: ```123```\",\n",
    "        model = DEFAULT_MODEL, #LLAMA2_70B\n",
    "    )\n",
    "    match = re.search(r'```(\\d+)```', response)\n",
    "    if match is None:\n",
    "        return None\n",
    "    return match.group(1)\n",
    "\n",
    "answers = [gen_answer() for i in range(5)]\n",
    "\n",
    "print(\n",
    "    f\"Answers: {answers}\\n\",\n",
    "    f\"Final answer: {mode(answers)}\",\n",
    "    )\n",
    "\n",
    "complete_and_print(\n",
    "    \"\"\"\n",
    "    # Python code to calculate: ((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "    \"\"\",\n",
    "    model=\"meta/codellama-34b:67942fd0f55b66da802218a19a8f0e1d73095473674061a6ea19f2dc8c053152\"\n",
    ")\n",
    "\n",
    "complete_and_print(\"\"\"\n",
    "Calculate the answer to the following math problem:\n",
    "\n",
    "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "\"\"\")\n",
    "\n",
    "MENLO_PARK_TEMPS = {\n",
    "    \"2023-12-11\": \"52 degrees Fahrenheit\",\n",
    "    \"2023-12-12\": \"51 degrees Fahrenheit\",\n",
    "    \"2023-12-13\": \"51 degrees Fahrenheit\",\n",
    "}\n",
    "\n",
    "\n",
    "def prompt_with_rag(retrived_info, question):\n",
    "    complete_and_print(\n",
    "        f\"Given the following information: '{retrived_info}', respond to: '{question}'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def ask_for_temperature(day):\n",
    "    temp_on_day = MENLO_PARK_TEMPS.get(day) or \"unknown temperature\"\n",
    "    prompt_with_rag(\n",
    "        f\"The temperature in Menlo Park was {temp_on_day} on {day}'\",  # Retrieved fact\n",
    "        f\"What is the temperature in Menlo Park on {day}?\",  # User question\n",
    "    )\n",
    "\n",
    "\n",
    "ask_for_temperature(\"2023-12-12\")\n",
    "# \"Sure! The temperature in Menlo Park on 2023-12-12 was 51 degrees Fahrenheit.\"\n",
    "\n",
    "ask_for_temperature(\"2023-07-18\")\n",
    "\n",
    "\n",
    "### Limiting Extraneous Tokens\n",
    "\n",
    "A common struggle is getting output without extraneous tokens (ex. \"Sure! Here's more information on...\").\n",
    "\n",
    "Check out this improvement that combines a role, rules and restrictions, explicit instructions, and an example:\n",
    "\n",
    "\n",
    "complete_and_print(\n",
    "    \"Give me the zip code for Menlo Park in JSON format with the field 'zip_code'\",\n",
    "    model = LLAMA3_70B_CHAT,\n",
    ")\n",
    "# Likely returns the JSON and also \"Sure! Here's the JSON...\"\n",
    "\n",
    "complete_and_print(\n",
    "    \"\"\"\n",
    "    You are a robot that only outputs JSON.\n",
    "    You reply in JSON format with the field 'zip_code'.\n",
    "    Example question: What is the zip code of the Empire State Building? Example answer: {'zip_code': 10118}\n",
    "    Now here is my question: What is the zip code of Menlo Park?\n",
    "    \"\"\",\n",
    "    model = LLAMA3_70B_CHAT,\n",
    ")\n",
    "# \"{'zip_code': 94025}\"\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92cf472e66798cee"
  },
  {
   "cell_type": "code",
   "source": [
    "securityChatbot = QuestionAndAnsweringCustomLlama3(QuestionRewritingPrompt=QuestionAndAnsweringCustomLlama2.QUESTION_REWRITING_TYPE.QUESTION_REWRITING_DEFAULT,\n",
    "                                 QuestionAnsweringPrompt=QuestionAndAnsweringCustomLlama3.SECURITY_PROMPT_TYPE.PROMPT_TYPE_SECURITY_OFFICER_WITH_RAG_MEMORY_NOSOURCES,\n",
    "                                 ModelType=QuestionAndAnsweringCustomLlama2.LLAMA2_VERSION_TYPE.LLAMA2_7B_chat,\n",
    "                                debug=True, streamingOnAnotherThread=True, demoMode=True, noInitialize=False)\n",
    "\n",
    "\n",
    "\n",
    "securityChatbot.test_vectorDatasets_similarityScores_and_responses_no_memory(run_llm_chain=False)\n",
    "\n",
    "securityChatbot.ask_question_and_streamtoconsole(\"What models use human instructions?\")\n",
    "securityChatbot.ask_question_and_streamtoconsole(\"Which are the advantage of each of these models?\")\n",
    "securityChatbot.ask_question_and_streamtoconsole(\"What are the downsides of your last model suggested above ?\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16ee333be016881f",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "securityChatbot.simulate_raq_question(\"can you show me a resource utilization graph comparison between a normal session and current situation?\", run_llm_chain=True)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c252f8e3a5ffa3cf",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "securityChatbot.simulate_raq_question(\"What Smart IOT applications are developed by Rares, Alin, and Ciprian Paduraru?\", run_llm_chain=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a550425f1f79efa",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "start=time.time()\n",
    "securityChatbot.ask_question(\"Give me some indications to solve a denial of service attack.\")\n",
    "securityChatbot.ask_question(\"Give me a few tools for this please\")\n",
    "securityChatbot.ask_question(\"Which one works best on Windows machines?\")\n",
    "\n",
    "\n",
    "end=time.time()\n",
    "\n",
    "print(f\"Total time: {end-start}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bea34d07bfde326",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
