{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T07:47:37.151159Z",
     "start_time": "2024-06-26T07:47:37.037524Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load a Llama-3-8B instruct \n",
    "import transformers\n",
    "import sys\n",
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from time import time\n",
    "#import chromadb\n",
    "#from chromadb.config import Settings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "import torch\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from typing import Union, List, Dict, Any, NoReturn\n",
    "from advancedRagUtils import convert_score_to_float\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee61a3db697c43ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T07:47:46.895874Z",
     "start_time": "2024-06-26T07:47:37.152055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6572a3c4e4dc4cfea86939b6875c22c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare model, tokenizer: 4.824 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The agent's memory is a long-term memory module that records a comprehensive list of agents' experiences in natural language, allowing the agent to retain and recall information over extended periods. This memory stream is composed of observations, events, and inter-agent communication, which are used to inform the agent's behavior. The retrieval model surfaces the context to guide the agent's future behavior, taking into account recency, importance, and relevance.\n",
      "answer is good for the question? {'score': 'yes'}\n",
      "halucination grader, i.e., is the answer sustained by documents? {'score': 'yes'}\n",
      "question category: {'datasource': 'vectorstore'})\n"
     ]
    }
   ],
   "source": [
    "# Test the AdaptiveRAGSupport\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from AdaptiveRAGSupport import AdaptiveRAGSupport, logger\n",
    "from pprint import pprint\n",
    "model = None\n",
    "tokenizer = None\n",
    "retriever = None\n",
    "support = AdaptiveRAGSupport(model, tokenizer, retriever)\n",
    "\n",
    "question = \"agent memory\" #\"What is the best way to train a model?\"\n",
    "docs_content = support.get_docs_content_by_query(question, support.DEFAULT_MAX_DOCS_RETURNED, format_as_single_text=True, verbose=False)\n",
    "answer = support.answer_with_rag(question, docs_content, skip_prompt=True)\n",
    "logger.warning(answer)\n",
    "logger.warning(f\"answer is good for the question? {support.answer_grader(question, answer, skip_prompt=True)}\")\n",
    "logger.warning(f\"halucination grader, i.e., is the answer sustained by documents? {support.hallucination_grader(answer, docs_content, skip_prompt=True)}\")\n",
    "logger.warning(f\"question category: {support.question_router(question, skip_prompt=True)})\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b78aa16cb3bc1878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "question category: {'datasource': 'vectorstore'})\n"
     ]
    }
   ],
   "source": [
    "logger.warning(f\"question category: {support.question_router('What are the types of agent memory?', skip_prompt=True)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "600ac2fcd5e09b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Search\n",
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a7a98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "class AgentMemory(TypedDict):\n",
    "    \"\"\"\n",
    "    Agent Memory to store the state of the agent and the conversation history \n",
    "    \"\"\"\n",
    "    \n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: bool\n",
    "    documents: List[str]\n",
    "    \n",
    "    \n",
    "def retrieve(state: AgentMemory):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "    Args:\n",
    "        state: AgentMemory\n",
    "    Returns:\n",
    "        state (dict): New keys to be added to the state/update the state, containing the documents\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    logger.warning(f\"Retrieving documents for question: {question}\")\n",
    "    \n",
    "    documents = support.get_docs_content_by_query(question, support.DEFAULT_MAX_DOCS_RETURNED, format_as_single_text=False, verbose=False)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "    \n",
    "def generate(state: AgentMemory):\n",
    "    \"\"\"\n",
    "    Generate an answer using RAG model\n",
    "    Args:\n",
    "        state: AgentMemory\n",
    "    Returns:\n",
    "        state (dict): New keys to be added to the state/update the state, containing the generation\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    logger.warning(f\"Generating answer for question: {question}\")\n",
    "    \n",
    "    answer = support.answer_with_rag(question, documents, skip_prompt=True)\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": answer}\n",
    "\n",
    "def grade_documents(state: AgentMemory):\n",
    "    \"\"\"\n",
    "    Checks if the response is sustained by the documents. If any doc is not relevant, we set a flag to run a web search\n",
    "    Args:\n",
    "        state: AgentMemory\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updates the web_search flag\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    logger.warning(f\"Grading documents for question: {question}\")\n",
    "    \n",
    "    # Check if the answer is sustained by the documents\n",
    "    # Score each individually\n",
    "    filtered_docs = []\n",
    "    for doc in documents:\n",
    "        score = support.hallucination_grader(generation, doc.page_content, skip_prompt=True)\n",
    "        \n",
    "        grade = convert_score_to_float(score)\n",
    "        \n",
    "        # Document is relevant if the grade is above 0.5\n",
    "        if grade > 0.5:\n",
    "            filtered_docs.append(doc)\n",
    "        # If not, we set a flag to run a web search\n",
    "        else: \n",
    "            web_search = True\n",
    "            continue # IS THIS NEEDED ?????\n",
    "        \n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "def web_search(state: AgentMemory):\n",
    "    \"\"\"\n",
    "    Run a web search based on the question\n",
    "    \n",
    "    Args:\n",
    "        state: AgentMemory\n",
    "    Returns:\n",
    "        state (dict): Added the web search results to the state\n",
    "        \n",
    "    \"\"\"\n",
    "    # We run a web search\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Web search \n",
    "    logger.warning(f\"Running web search for question: {question}\")\n",
    "    web_search_docs = web_search_tool.invoke({\"query\": question, 'k': support.NUM_WEB_SEARCH_RESULTS})\n",
    "    web_results = \"\\n\".join(doc[\"content\"] for doc in web_search_docs)\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:    \n",
    "        documents = [web_results]\n",
    "        \n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "# Conditional edges\n",
    "def route_question(state: AgentMemory):\n",
    "    \"\"\"\n",
    "    Route the question to the right category, RAG or websearch\n",
    "    Args:\n",
    "        state: AgentMemory\n",
    "    Returns:\n",
    "        str: name of the next node to be executed\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    \n",
    "    logger.warning(f\"Routing question: {question}\")\n",
    "    \n",
    "    source = support.question_router(question, skip_prompt=True)\n",
    "    \n",
    "    logger.warning(f\"Question category: {source['datasource']}\")\n",
    "    if source[\"datasource\"] == \"websearch\":\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        return \"vectorstore\"\n",
    "    \n",
    "    \n",
    "    category = support.question_router(question, generation, documents, skip_prompt=True)\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation, \"category\": category}\n",
    "\n",
    "def decide_if_websearch_is_needed(state: AgentMemory):\n",
    "    \"\"\"\n",
    "    Check if we need to run the web search after a collection of documents were retrieved based on the state of the agent\n",
    "    Args:\n",
    "        state: AgentMemory\n",
    "    Returns:\n",
    "        str: name of the next node to be executed\n",
    "    \"\"\"\n",
    "    web_search = state[\"web_search\"]\n",
    "    logger.warning(f\"Assessing the status of web search: {web_search}\")\n",
    "    assert type(web_search) == bool, \"web_search should be a boolean\"\n",
    "    if web_search:\n",
    "        logger.warning(f\"---DECISION: Not all documents are relevant, running web search\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        logger.warning(f\"---DECISION: ALL documents are relevant, go to generation\")\n",
    "        return \"vectorstore\"\n",
    "    \n",
    "def grade_generation_vs_documents_and_question(state: AgentMemory):\n",
    "    \"\"\"\n",
    "    Determines if the generation is actually grounded in the documents retrieved from the vectorstore according and answering the question\n",
    "    Args:\n",
    "        state: AgentMemory\n",
    "    Returns:\n",
    "        str: Decision for next node to call.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    \n",
    "    logger.warning(f\"Hallucination check and then if the answer is useful: Grading generation vs documents and question\")\n",
    "    \n",
    "    # Check if answer is sustained by the documents, hallucination check\n",
    "    score_hallucination = support.hallucination_grader(question, documents, skip_prompt=True)\n",
    "    grade_hallucination = convert_score_to_float(score_hallucination)\n",
    "    if grade_hallucination > 0.5:\n",
    "        # Answer is sustained by the documents, now check if the answer is useful\n",
    "        logger.warning(f\"---DECISION: Generation is grounded in the documents\")\n",
    "                    \n",
    "        score_useful = support.answer_grader(question, generation, skip_prompt=True)\n",
    "        grade_useful = convert_score_to_float(score_useful)\n",
    "        if grade_useful > 0.5:\n",
    "            logger.warning(f\"---DECISION: Generation is sustained by the documents and the answer is useful\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            logger.warning(f\"---DECISION: Generation does not address question or is not useful\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        logger.warning(f\"---DECISION: Generation is NOT grounded in the documents, RETRYING\")\n",
    "        return \"not supported\"\n",
    "    \n",
    "workflow = StateGraph(AgentMemory)\n",
    "\n",
    "workflow.add_node(\"websearch\", web_search) # web search node\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve node\n",
    "workflow.add_node(\"generate\", generate) # generate node\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents node\n",
    "\n",
    "# workflow.add_node(\"route_question\", route_question) # route question node\n",
    "# workflow.add_node(\"decide_if_websearch_is_needed\", decide_if_websearch_is_needed) # decide if websearch is needed node\n",
    "# workflow.add_node(\"grade_generation_vs_documents_and_question\", grade_generation_vs_documents_and_question) # grade generation vs documents and question node\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa231f23ef7b2e60",
   "metadata": {},
   "source": [
    "## Building the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7dc59266bc98d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result \n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\" : \"websearch\",\n",
    "        \"vectorstore\" : \"retrieve\",\n",
    "    },\n",
    ") # Dict from result of the function to next node\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_if_websearch_is_needed,\n",
    "    {\n",
    "        \"websearch\" : \"websearch\",\n",
    "        \"generate\" : \"generate\", \n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_vs_documents_and_question,\n",
    "    {\n",
    "      \"not supported\" : \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61972002d557530c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T07:47:58.449623Z",
     "start_time": "2024-06-26T07:47:57.551077Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Routing question: What are the types of agent memory?\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Question category: vectorstore\n",
      "Retrieving documents for question: What are the types of agent memory?\n",
      "Grading documents for question: What are the types of agent memory?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Finished running: retrieve:'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assessing the status of web search: True\n",
      "---DECISION: Not all documents are relevant, running web search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Finished running: grade_documents:'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running web search for question: What are the types of agent memory?\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Test \u001b[39;00m\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the types of agent memory?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream(inputs):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      9\u001b[0m         pprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished running: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langgraph/pregel/__init__.py:983\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 983\u001b[0m _panic_or_proceed(done, inflight, step)\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1537\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1535\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1536\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1537\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langgraph/pregel/retry.py:72\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     70\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, task\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:2502\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2498\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   2499\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2500\u001b[0m )\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2502\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2504\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langgraph/utils.py:95\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m     94\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m---> 95\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[5], line 94\u001b[0m, in \u001b[0;36mweb_search\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     92\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning web search for question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m web_search_docs \u001b[38;5;241m=\u001b[39m web_search_tool\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: question})\n\u001b[0;32m---> 94\u001b[0m web_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m web_search_docs)\n\u001b[1;32m     95\u001b[0m web_results \u001b[38;5;241m=\u001b[39m Document(content\u001b[38;5;241m=\u001b[39mweb_results)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[5], line 94\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     92\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning web search for question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m web_search_docs \u001b[38;5;241m=\u001b[39m web_search_tool\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: question})\n\u001b[0;32m---> 94\u001b[0m web_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m web_search_docs)\n\u001b[1;32m     95\u001b[0m web_results \u001b[38;5;241m=\u001b[39m Document(content\u001b[38;5;241m=\u001b[39mweb_results)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'page_content'"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test \n",
    "\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6263bf30dbe191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
