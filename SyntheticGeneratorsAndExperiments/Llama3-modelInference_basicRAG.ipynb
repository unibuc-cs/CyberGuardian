{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce4203ee6697f8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T12:29:49.240480Z",
     "start_time": "2024-06-20T12:28:30.173366Z"
    }
   },
   "source": [
    "\n",
    "# Load a Llama-3-8B instruct \n",
    "import transformers\n",
    "import sys\n",
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from time import time\n",
    "#import chromadb\n",
    "#from chromadb.config import Settings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "time_start = time()\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "   model_id,\n",
    "    trust_remote_code=True,\n",
    "    #max_new_tokens=1024\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "time_end = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb7bda9",
   "metadata": {},
   "source": [
    "# Create a query pipeline using transformers and test it!\n",
    "time_start = time()\n",
    "query_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        #max_length=1024,\n",
    "        device_map=\"auto\",)\n",
    "time_end = time()\n",
    "print(f\"Prepare pipeline: {round(time_end-time_start, 3)} sec.\")\n",
    "\n",
    "\n",
    "# Test the model and pipeline \n",
    "def test_model(tokenizer, pipeline, message):\n",
    "    \"\"\"\n",
    "    Perform a query\n",
    "    print the result\n",
    "    Args:\n",
    "        tokenizer: the tokenizer\n",
    "        pipeline: the pipeline\n",
    "        message: the prompt\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"    \n",
    "    time_start = time()\n",
    "    sequences = pipeline(\n",
    "        message,\n",
    "        do_sample=True,\n",
    "        temperature=2.3,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,)\n",
    "    time_end = time()\n",
    "    total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
    "    \n",
    "    question = sequences[0]['generated_text'][:len(message)]\n",
    "    answer = sequences[0]['generated_text'][len(message):]\n",
    "    \n",
    "    return f\"Question: {question}\\nAnswer: {answer}\\nTotal time: {total_time}\"\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text\n",
    "\n",
    "response = test_model(tokenizer,\n",
    "                    query_pipeline,\n",
    "                   \"Please explain what is EU AI Act.\")\n",
    "display(Markdown(colorize_text(response)))\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a01ab0d",
   "metadata": {},
   "source": [
    "# Create and test the HF pipeline\n",
    "# checking again that everything is working fine\n",
    "\n",
    "query_pipeline_hf = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "\n",
    "time_start = time()\n",
    "question = \"Please explain what EU AI Act is.\"\n",
    "response = query_pipeline_hf(prompt=question)\n",
    "time_end = time()\n",
    "total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
    "full_response =  f\"Question: {question}\\nAnswer: {response}\\nTotal time: {total_time}\"\n",
    "display(Markdown(colorize_text(full_response)))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4431fa33",
   "metadata": {},
   "source": [
    "# Create a database of knowledge with ChromaDB and PyPDF Loader \n",
    "loader = PyPDFLoader(\"./aiact_final_draft.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Chunk overlapping is required in order to be able to keep the context, even if we have a concept that we want to include that is spread over multiple document chunks.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "# try to access the sentence transformers from HuggingFace: https://huggingface.co/api/models/sentence-transformers/all-mpnet-base-v2\n",
    "try:\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "except Exception as ex:\n",
    "    print(\"Exception: \", ex)\n",
    "    # alternatively, we will access the embeddings models locally\n",
    "    local_model_path = \"/kaggle/input/sentence-transformers/minilm-l6-v2/all-MiniLM-L6-v2\"\n",
    "    print(f\"Use alternative (local) model: {local_model_path}\\n\")\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=local_model_path, model_kwargs=model_kwargs)\n",
    "    \n",
    "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8e7024",
   "metadata": {},
   "source": [
    "# Create and test a Retrieval QA chain with with plain code and test it\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=query_pipeline_hf, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def test_rag(qa_chain, query):\n",
    "    time_start = time()\n",
    "    response = qa_chain.run(query)\n",
    "    time_end = time()\n",
    "    total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
    "\n",
    "    full_response =  f\"Question: {query}\\nAnswer: {response}\\nTotal time: {total_time}\"\n",
    "    display(Markdown(colorize_text(full_response)))\n",
    "    \n",
    "query = \"How is performed the testing of high-risk AI systems in real world conditions?\"\n",
    "test_rag(qa_chain, query)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6178d467",
   "metadata": {},
   "source": [
    "query = \"What are the operational obligations of notified bodies?\"\n",
    "test_rag(qa_chain, query)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f48d3f",
   "metadata": {},
   "source": [
    "# Check the document sources for the last query\n",
    "docs = vectordb.similarity_search(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved documents: {len(docs)}\")\n",
    "for doc in docs:\n",
    "    doc_details = doc.to_json()['kwargs']\n",
    "    print(\"Source: \", doc_details['metadata']['source'])\n",
    "    print(\"Text: \", doc_details['page_content'], \"\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1a46ab34",
   "metadata": {},
   "source": [
    "### Tracing (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "333cbcf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T14:45:48.611947Z",
     "start_time": "2024-06-20T14:45:48.609670Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096edbdb",
   "metadata": {},
   "source": [
    "\n",
    "# Load a Llama-3-8B instruct \n",
    "import transformers\n",
    "import sys\n",
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from time import time\n",
    "#import chromadb\n",
    "#from chromadb.config import Settings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "time_start = time()\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "   model_id,\n",
    "    trust_remote_code=True,\n",
    "    #max_new_tokens=1024\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "time_end = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b34dcc",
   "metadata": {},
   "source": [
    "# Create a query pipeline using transformers and test it!\n",
    "time_start = time()\n",
    "query_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        #max_length=1024,\n",
    "        device_map=\"auto\",)\n",
    "time_end = time()\n",
    "print(f\"Prepare pipeline: {round(time_end-time_start, 3)} sec.\")\n",
    "\n",
    "\n",
    "# Test the model and pipeline \n",
    "def test_model(tokenizer, pipeline, message):\n",
    "    \"\"\"\n",
    "    Perform a query\n",
    "    print the result\n",
    "    Args:\n",
    "        tokenizer: the tokenizer\n",
    "        pipeline: the pipeline\n",
    "        message: the prompt\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"    \n",
    "    time_start = time()\n",
    "    sequences = pipeline(\n",
    "        message,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,)\n",
    "    time_end = time()\n",
    "    total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
    "    \n",
    "    question = sequences[0]['generated_text'][:len(message)]\n",
    "    answer = sequences[0]['generated_text'][len(message):]\n",
    "    \n",
    "    return f\"Question: {question}\\nAnswer: {answer}\\nTotal time: {total_time}\"\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text\n",
    "\n",
    "response = test_model(tokenizer,\n",
    "                    query_pipeline,\n",
    "                   \"Please explain what is EU AI Act.\")\n",
    "display(Markdown(colorize_text(response)))\n",
    "\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
