{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b7eef9-23a4-430a-bc89-d3002293e276",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PROMPT_TYPE_DEFAULT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     30\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mQuestionAndAnswerUtils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtest\u001b[39;00m\n",
      "File \u001b[0;32m~/dynabicChatbot/QuestionAndAnswerUtils.py:34\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mQuestionAndAnsweringCustomLlama2\u001b[39;00m():\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSECURITY_PROMPT_TYPE\u001b[39;00m(Enum):\n\u001b[1;32m     37\u001b[0m         PROMPT_TYPE_DEFAULT,\n",
      "File \u001b[0;32m~/dynabicChatbot/QuestionAndAnswerUtils.py:36\u001b[0m, in \u001b[0;36mQuestionAndAnsweringCustomLlama2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mQuestionAndAnsweringCustomLlama2\u001b[39;00m():\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSECURITY_PROMPT_TYPE\u001b[39;00m(Enum):\n\u001b[1;32m     37\u001b[0m         PROMPT_TYPE_DEFAULT,\n\u001b[1;32m     38\u001b[0m         PROMPT_TYPE_SECURITY_OFFICER_WITH_RAG_MEMORY_NOSOURCES,\n",
      "File \u001b[0;32m~/dynabicChatbot/QuestionAndAnswerUtils.py:37\u001b[0m, in \u001b[0;36mQuestionAndAnsweringCustomLlama2.SECURITY_PROMPT_TYPE\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSECURITY_PROMPT_TYPE\u001b[39;00m(Enum):\n\u001b[0;32m---> 37\u001b[0m     PROMPT_TYPE_DEFAULT,\n\u001b[1;32m     38\u001b[0m     PROMPT_TYPE_SECURITY_OFFICER_WITH_RAG_MEMORY_NOSOURCES,\n\u001b[1;32m     39\u001b[0m     PROMPT_TYPE_SECURITY_OFFICER_WITH_RAG_MEMORY_WITHSOURCES,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PROMPT_TYPE_DEFAULT' is not defined"
     ]
    }
   ],
   "source": [
    "import secrets\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import pdb\n",
    "from typing import Any\n",
    "\n",
    "from etl import markdown, pdfs, shared, videos\n",
    "\n",
    "import docstore\n",
    "import vecstore\n",
    "from utils import pretty_log\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline, TextStreamer\n",
    "import json\n",
    "import textwrap\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import langchain\n",
    "import time\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "import QuestionAndAnswerUtils as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b544796-7749-4ab5-888d-476eb2feb652",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T08:51:50.790164600Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ce2db97ed54ecb991f1e895a44c8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mðŸ¥ž: connecting to vector storage \u001b[0m\n",
      "\u001b[1;38;5;214mðŸ¥ž: connected to vector storage \u001b[0m\n",
      "\u001b[1;38;5;214mðŸ¥ž: found 540 vectors to search over \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline, TextStreamer\n",
    "import json\n",
    "import textwrap\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-13b-chat-hf\" # \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "embedding_engine = vecstore.get_embedding_engine(allowed_special=\"all\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          token=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             token=True,\n",
    "                                             #  load_in_8bit=True,\n",
    "                                             #  load_in_4bit=True,\n",
    "                                             )\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens=4096,\n",
    "                do_sample=True,\n",
    "                # temperature=0.1,\n",
    "                top_p=0.95,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                streamer=streamer,\n",
    "                )\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "\n",
    "pretty_log(\"connecting to vector storage\")\n",
    "vector_index = vecstore.connect_to_vector_index(vecstore.INDEX_NAME, embedding_engine)\n",
    "pretty_log(\"connected to vector storage\")\n",
    "pretty_log(f\"found {vector_index.index.ntotal} vectors to search over\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45dd9d8-6f8f-4805-9935-664c46317b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########## THE LAMA 2 DEMO ##############\n",
    "\n",
    "template = \"\"\"\n",
    "[INST]Use the following pieces of context to answer the question. If no context provided, answer like a AI assistant.\n",
    "{context}\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "retriever = vector_index.as_retriever(\n",
    "        search_kwargs={\"k\": 6}\n",
    "    )\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,     \n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "110e5a85-5a0e-48dc-a0f1-2f0f777080fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the conversation, it appears that the speaker is discussing the use of large language models in artificial intelligence (AI) and the potential for these models to learn from human instructions. The speaker mentions that these models can be trained with human feedback and can improve exponentially with the use of reinforcement learning. They also mention that the user becomes the most valuable piece of the equation and that people will be able to do things they were never able to before.\n",
      "\n",
      "In terms of specific models that use human instructions, the speaker does not provide any specific examples. However, they do mention the use of reinforcement learning with human feedback to train these models. This suggests that the models may be using some form of supervised learning or deep reinforcement learning, which involves using human instructions or feedback to guide the learning process.\n",
      "\n",
      "Overall, the conversation seems to be focused on the potential of large language models and the use of human instructions to train and improve these models. The speaker is enthusiastic about the possibilities of these technologies and sees them as having the potential to democratize AI and make it more accessible to a wider range of people.</s>\n",
      "{'query': 'What models use human instructions?', 'result': '  Based on the conversation, it appears that the speaker is discussing the use of large language models in artificial intelligence (AI) and the potential for these models to learn from human instructions. The speaker mentions that these models can be trained with human feedback and can improve exponentially with the use of reinforcement learning. They also mention that the user becomes the most valuable piece of the equation and that people will be able to do things they were never able to before.\\n\\nIn terms of specific models that use human instructions, the speaker does not provide any specific examples. However, they do mention the use of reinforcement learning with human feedback to train these models. This suggests that the models may be using some form of supervised learning or deep reinforcement learning, which involves using human instructions or feedback to guide the learning process.\\n\\nOverall, the conversation seems to be focused on the potential of large language models and the use of human instructions to train and improve these models. The speaker is enthusiastic about the possibilities of these technologies and sees them as having the potential to democratize AI and make it more accessible to a wider range of people.'}\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": \"What models use human instructions?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "591e0b55-dedb-4553-b286-3dd1ac943eeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T15:54:44.754946900Z",
     "start_time": "2024-01-11T15:54:44.749947200Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "########## THE LAMA 2 DEMO ############## - MORE GENERIC AND CUSTOMIZED \n",
    "langchain.debug=False \n",
    "\n",
    "llama_docs_template = \"\"\"\n",
    "[INST]Use the following pieces of context to answer the question. If no context provided, answer like a AI assistant.\n",
    "{context}\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "llama_docs_prompt = PromptTemplate(template=llama_docs_template, input_variables=[\"context\", \"question\"])\n",
    "llama_doc_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", prompt= llama_docs_prompt, document_variable_name=\"context\", verbose=False)\n",
    "\n",
    "llama_condense_template = \"\"\"\n",
    "[INST]Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question: [/INST]\"\"\"\n",
    "llama_condense_prompt = PromptTemplate(template=llama_condense_template, input_variables=[\"chat_history\", \"question\"])\n",
    "llama_question_generator_chain = LLMChain(llm=llm, prompt=llama_condense_prompt, verbose=False)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "llama_v2_chain = ConversationalRetrievalChain(\n",
    "    retriever=vector_index.as_retriever(search_kwargs={'k': 6}),\n",
    "    question_generator=llama_question_generator_chain,\n",
    "    combine_docs_chain=llama_doc_chain,\n",
    "    memory=memory\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae01051-b5ea-47b2-a521-291ed6771156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T15:55:29.123768700Z",
     "start_time": "2024-01-11T15:54:49.735822400Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided content, it appears that the speaker is discussing the use of large language models and reinforcement learning with human feedback to improve the performance of AI systems. The speaker mentions that these models can learn exponentially better with human feedback and that the user becomes the most valuable piece of the equation.\n",
      "\n",
      "To answer your question, some examples of AI models that use human instructions include:\n",
      "\n",
      "1. Reinforcement learning models: These models use human feedback in the form of rewards or penalties to learn and improve their performance.\n",
      "2. Deep learning models: These models use human-labeled data to learn and improve their performance.\n",
      "3. Natural Language Processing (NLP) models: These models use human-provided instructions to perform tasks such as text summarization, sentiment analysis, and language translation.\n",
      "\n",
      "It's worth noting that the speaker's comments suggest that the use of human instructions and feedback is becoming increasingly important in the development and improvement of AI systems.</s>\n",
      "{'question': 'What models use human instructions?', 'chat_history': [HumanMessage(content='What models use human instructions?'), AIMessage(content=\"  Based on the provided content, it appears that the speaker is discussing the use of large language models and reinforcement learning with human feedback to improve the performance of AI systems. The speaker mentions that these models can learn exponentially better with human feedback and that the user becomes the most valuable piece of the equation.\\n\\nTo answer your question, some examples of AI models that use human instructions include:\\n\\n1. Reinforcement learning models: These models use human feedback in the form of rewards or penalties to learn and improve their performance.\\n2. Deep learning models: These models use human-labeled data to learn and improve their performance.\\n3. Natural Language Processing (NLP) models: These models use human-provided instructions to perform tasks such as text summarization, sentiment analysis, and language translation.\\n\\nIt's worth noting that the speaker's comments suggest that the use of human instructions and feedback is becoming increasingly important in the development and improvement of AI systems.\")], 'answer': \"  Based on the provided content, it appears that the speaker is discussing the use of large language models and reinforcement learning with human feedback to improve the performance of AI systems. The speaker mentions that these models can learn exponentially better with human feedback and that the user becomes the most valuable piece of the equation.\\n\\nTo answer your question, some examples of AI models that use human instructions include:\\n\\n1. Reinforcement learning models: These models use human feedback in the form of rewards or penalties to learn and improve their performance.\\n2. Deep learning models: These models use human-labeled data to learn and improve their performance.\\n3. Natural Language Processing (NLP) models: These models use human-provided instructions to perform tasks such as text summarization, sentiment analysis, and language translation.\\n\\nIt's worth noting that the speaker's comments suggest that the use of human instructions and feedback is becoming increasingly important in the development and improvement of AI systems.\"}\n",
      " Sure! Here's the standalone question based on the follow-up input:\n",
      "\n",
      "What are the advantages of reinforcement learning models, deep learning models, and natural language processing (NLP) models that use human instructions and feedback?</s>\n",
      "\n",
      "Content: Sure, I'd be happy to help! Based on the input provided, here's the standalone question:\n",
      "\n",
      "What are the advantages of reinforcement learning models, deep learning models, and natural language processing (NLP) models that use human instructions and feedback?\n",
      "\n",
      "Context: The input provided mentions the use of human instructions and feedback in reinforcement learning models, deep learning models, and natural language processing (NLP) models. It also mentions the potential advantages of using these models in various applications, such as training AI models, improving the performance of AI systems, and enhancing the capabilities of AI models.\n",
      "\n",
      "Question: What are the advantages of using human instructions and feedback in reinforcement learning models, deep learning models, and NLP models? How can these models be used to improve the performance of AI systems and enhance their capabilities?\n",
      "\n",
      "Possible answers:\n",
      "\n",
      "1. Improved accuracy and performance: By using human instructions and feedback, reinforcement learning models, deep learning models, and NLP models can improve their accuracy and performance in specific tasks and applications.\n",
      "2. Increased efficiency and effectiveness: Human instructions and feedback can help these models learn more efficiently and effectively, reducing the need for large amounts of data and computational resources.\n",
      "3. Enhanced adaptability and flexibility: These models can adapt and learn from human feedback, allowing them to better handle unexpected situations and improve their overall performance.\n",
      "4. Improved interpretability and explainability: By using human instructions and feedback, these models can provide more interpretable and explainable results, allowing users to better understand the reasoning behind their predictions and decisions.\n",
      "5. Increased safety and security: By using human feedback and instructions, these models can be trained to avoid certain types of errors and biases, improving their safety and security in critical applications.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions or need further clarification.</s>\n",
      "{'question': 'Which are the advantage of each of these models?', 'chat_history': [HumanMessage(content='What models use human instructions?'), AIMessage(content=\"  Based on the provided content, it appears that the speaker is discussing the use of large language models and reinforcement learning with human feedback to improve the performance of AI systems. The speaker mentions that these models can learn exponentially better with human feedback and that the user becomes the most valuable piece of the equation.\\n\\nTo answer your question, some examples of AI models that use human instructions include:\\n\\n1. Reinforcement learning models: These models use human feedback in the form of rewards or penalties to learn and improve their performance.\\n2. Deep learning models: These models use human-labeled data to learn and improve their performance.\\n3. Natural Language Processing (NLP) models: These models use human-provided instructions to perform tasks such as text summarization, sentiment analysis, and language translation.\\n\\nIt's worth noting that the speaker's comments suggest that the use of human instructions and feedback is becoming increasingly important in the development and improvement of AI systems.\"), HumanMessage(content='Which are the advantage of each of these models?'), AIMessage(content=\"\\nContent: Sure, I'd be happy to help! Based on the input provided, here's the standalone question:\\n\\nWhat are the advantages of reinforcement learning models, deep learning models, and natural language processing (NLP) models that use human instructions and feedback?\\n\\nContext: The input provided mentions the use of human instructions and feedback in reinforcement learning models, deep learning models, and natural language processing (NLP) models. It also mentions the potential advantages of using these models in various applications, such as training AI models, improving the performance of AI systems, and enhancing the capabilities of AI models.\\n\\nQuestion: What are the advantages of using human instructions and feedback in reinforcement learning models, deep learning models, and NLP models? How can these models be used to improve the performance of AI systems and enhance their capabilities?\\n\\nPossible answers:\\n\\n1. Improved accuracy and performance: By using human instructions and feedback, reinforcement learning models, deep learning models, and NLP models can improve their accuracy and performance in specific tasks and applications.\\n2. Increased efficiency and effectiveness: Human instructions and feedback can help these models learn more efficiently and effectively, reducing the need for large amounts of data and computational resources.\\n3. Enhanced adaptability and flexibility: These models can adapt and learn from human feedback, allowing them to better handle unexpected situations and improve their overall performance.\\n4. Improved interpretability and explainability: By using human instructions and feedback, these models can provide more interpretable and explainable results, allowing users to better understand the reasoning behind their predictions and decisions.\\n5. Increased safety and security: By using human feedback and instructions, these models can be trained to avoid certain types of errors and biases, improving their safety and security in critical applications.\\n\\nI hope this helps! Let me know if you have any other questions or need further clarification.\")], 'answer': \"\\nContent: Sure, I'd be happy to help! Based on the input provided, here's the standalone question:\\n\\nWhat are the advantages of reinforcement learning models, deep learning models, and natural language processing (NLP) models that use human instructions and feedback?\\n\\nContext: The input provided mentions the use of human instructions and feedback in reinforcement learning models, deep learning models, and natural language processing (NLP) models. It also mentions the potential advantages of using these models in various applications, such as training AI models, improving the performance of AI systems, and enhancing the capabilities of AI models.\\n\\nQuestion: What are the advantages of using human instructions and feedback in reinforcement learning models, deep learning models, and NLP models? How can these models be used to improve the performance of AI systems and enhance their capabilities?\\n\\nPossible answers:\\n\\n1. Improved accuracy and performance: By using human instructions and feedback, reinforcement learning models, deep learning models, and NLP models can improve their accuracy and performance in specific tasks and applications.\\n2. Increased efficiency and effectiveness: Human instructions and feedback can help these models learn more efficiently and effectively, reducing the need for large amounts of data and computational resources.\\n3. Enhanced adaptability and flexibility: These models can adapt and learn from human feedback, allowing them to better handle unexpected situations and improve their overall performance.\\n4. Improved interpretability and explainability: By using human instructions and feedback, these models can provide more interpretable and explainable results, allowing users to better understand the reasoning behind their predictions and decisions.\\n5. Increased safety and security: By using human feedback and instructions, these models can be trained to avoid certain types of errors and biases, improving their safety and security in critical applications.\\n\\nI hope this helps! Let me know if you have any other questions or need further clarification.\"}\n",
      " Sure! Here's the standalone question based on the follow-up input:\n",
      "\n",
      "What are the potential downsides or limitations of using human instructions and feedback in reinforcement learning models, deep learning models, and natural language processing (NLP) models?\n",
      "\n",
      "Context: The input provided discusses the advantages of using human instructions and feedback in these models, but it is also important to consider the potential downsides or limitations of this approach.\n",
      "\n",
      "Question: What are the potential downsides or limitations of using human instructions and feedback in reinforcement learning models, deep learning models, and NLP models? How might these limitations impact the performance and effectiveness of AI systems that rely on these models?\n",
      "\n",
      "Possible answers:\n",
      "\n",
      "1. Limited generalizability: Human instructions and feedback may not be applicable to all situations and tasks, limiting the generalizability of these models.\n",
      "2. Bias and errors: Human feedback can introduce bias and errors into these models, particularly if the feedback is not carefully curated.\n",
      "3. Dependence on human expertise: These models may be limited by the expertise and knowledge of the humans providing feedback, which may not be comprehensive or up-to-date.\n",
      "4. Time-consuming and costly: Obtaining high-quality human feedback can be time-consuming and costly, particularly for complex tasks and applications.\n",
      "5. Difficulty in scaling: As the size and complexity of the models increase, it may become more difficult to obtain and incorporate human feedback, potentially limiting their scalability.</s>\n",
      "Content: what people are working on. - Yeah, that, I mean I think that's one of the things we\n",
      "heard when we were out west is that this, this changes the way anyone\n",
      "can interact with a computer. Whether it's someone who wants to write a\n",
      "technical piece of code or someone who just kind\n",
      "of wants to take an idea and turn it into something that\n",
      "can be deployed technically. - I mean the interesting,\n",
      "Source: <https://www.youtube.com/watch?v=Zo5M1lrMU1I&t=0s>\n",
      "\n",
      "Content: what people are working on. - Yeah, that, I mean I think that's one of the things we\n",
      "heard when we were out west is that this, this changes the way anyone\n",
      "can interact with a computer. Whether it's someone who wants to write a\n",
      "technical piece of code or someone who just kind\n",
      "of wants to take an idea and turn it into something that\n",
      "can be deployed technically. - I mean the interesting,\n",
      "and I'm really enjoying this, I talk about my, you know, back to the neural network\n",
      "with Professor Lerner was that you know, so much of the work historically went into training, you\n",
      "know, or into labeling data. Obviously the deep\n",
      "learning quickly allowed us to label, auto label data\n",
      "and train these models. But what I didn't realize is\n",
      "this reinforcement learning with human feedback and different versions of that, you know, if you can get an expert\n",
      "to react to the output of one of these models, you can immediately use that feedback and the model gets exponentially better, not just a little bit better. And so all of a sudden the user becomes the most valuable\n",
      "piece of the equation. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's why I'm actually not\n",
      "worried about the labor questions in many ways. I actually think people\n",
      "are gonna find this remarkably democratic and democratizing 'cause they're gonna be able to do things they were never able to, and they in fact will add what they know that nobody else knew. And the model will get that\n",
      "much better that quickly. But that's not the way we write software, and that's what, that's the work that's gotta be done. - Right. - Because you can't just say, okay, let's use it like an operating system. That isn't, like that's not works. - Interesting. We were talking a little bit earlier about how as products of\n",
      "Yale, you have a kind of a latent humanism that is\n",
      "kind of baked into your brain regardless of what you end up doing. What, you know, as we\n",
      "think about strategically what makes the most sense for us to be doubling down on here, how do you see your own experience at Yale, and the work that you've been doing, and the things that you've been doing. - [Audience]  Yeah, yeah, yeah, I've been doing.</s>\n",
      "{'question': 'What are the downsides of your last model suggested above ?', 'chat_history': [HumanMessage(content='What models use human instructions?'), AIMessage(content=\"  Based on the provided content, it appears that the speaker is discussing the use of large language models and reinforcement learning with human feedback to improve the performance of AI systems. The speaker mentions that these models can learn exponentially better with human feedback and that the user becomes the most valuable piece of the equation.\\n\\nTo answer your question, some examples of AI models that use human instructions include:\\n\\n1. Reinforcement learning models: These models use human feedback in the form of rewards or penalties to learn and improve their performance.\\n2. Deep learning models: These models use human-labeled data to learn and improve their performance.\\n3. Natural Language Processing (NLP) models: These models use human-provided instructions to perform tasks such as text summarization, sentiment analysis, and language translation.\\n\\nIt's worth noting that the speaker's comments suggest that the use of human instructions and feedback is becoming increasingly important in the development and improvement of AI systems.\"), HumanMessage(content='Which are the advantage of each of these models?'), AIMessage(content=\"\\nContent: Sure, I'd be happy to help! Based on the input provided, here's the standalone question:\\n\\nWhat are the advantages of reinforcement learning models, deep learning models, and natural language processing (NLP) models that use human instructions and feedback?\\n\\nContext: The input provided mentions the use of human instructions and feedback in reinforcement learning models, deep learning models, and natural language processing (NLP) models. It also mentions the potential advantages of using these models in various applications, such as training AI models, improving the performance of AI systems, and enhancing the capabilities of AI models.\\n\\nQuestion: What are the advantages of using human instructions and feedback in reinforcement learning models, deep learning models, and NLP models? How can these models be used to improve the performance of AI systems and enhance their capabilities?\\n\\nPossible answers:\\n\\n1. Improved accuracy and performance: By using human instructions and feedback, reinforcement learning models, deep learning models, and NLP models can improve their accuracy and performance in specific tasks and applications.\\n2. Increased efficiency and effectiveness: Human instructions and feedback can help these models learn more efficiently and effectively, reducing the need for large amounts of data and computational resources.\\n3. Enhanced adaptability and flexibility: These models can adapt and learn from human feedback, allowing them to better handle unexpected situations and improve their overall performance.\\n4. Improved interpretability and explainability: By using human instructions and feedback, these models can provide more interpretable and explainable results, allowing users to better understand the reasoning behind their predictions and decisions.\\n5. Increased safety and security: By using human feedback and instructions, these models can be trained to avoid certain types of errors and biases, improving their safety and security in critical applications.\\n\\nI hope this helps! Let me know if you have any other questions or need further clarification.\"), HumanMessage(content='What are the downsides of your last model suggested above ?'), AIMessage(content=\"Content: what people are working on. - Yeah, that, I mean I think that's one of the things we\\nheard when we were out west is that this, this changes the way anyone\\ncan interact with a computer. Whether it's someone who wants to write a\\ntechnical piece of code or someone who just kind\\nof wants to take an idea and turn it into something that\\ncan be deployed technically. - I mean the interesting,\\nSource: <https://www.youtube.com/watch?v=Zo5M1lrMU1I&t=0s>\\n\\nContent: what people are working on. - Yeah, that, I mean I think that's one of the things we\\nheard when we were out west is that this, this changes the way anyone\\ncan interact with a computer. Whether it's someone who wants to write a\\ntechnical piece of code or someone who just kind\\nof wants to take an idea and turn it into something that\\ncan be deployed technically. - I mean the interesting,\\nand I'm really enjoying this, I talk about my, you know, back to the neural network\\nwith Professor Lerner was that you know, so much of the work historically went into training, you\\nknow, or into labeling data. Obviously the deep\\nlearning quickly allowed us to label, auto label data\\nand train these models. But what I didn't realize is\\nthis reinforcement learning with human feedback and different versions of that, you know, if you can get an expert\\nto react to the output of one of these models, you can immediately use that feedback and the model gets exponentially better, not just a little bit better. And so all of a sudden the user becomes the most valuable\\npiece of the equation. It's why I'm actually not\\nworried about the labor questions in many ways. I actually think people\\nare gonna find this remarkably democratic and democratizing 'cause they're gonna be able to do things they were never able to, and they in fact will add what they know that nobody else knew. And the model will get that\\nmuch better that quickly. But that's not the way we write software, and that's what, that's the work that's gotta be done. - Right. - Because you can't just say, okay, let's use it like an operating system. That isn't, like that's not works. - Interesting. We were talking a little bit earlier about how as products of\\nYale, you have a kind of a latent humanism that is\\nkind of baked into your brain regardless of what you end up doing. What, you know, as we\\nthink about strategically what makes the most sense for us to be doubling down on here, how do you see your own experience at Yale, and the work that you've been doing, and the things that you've been doing. - [Audience]  Yeah, yeah, yeah, I've been doing.\")], 'answer': \"Content: what people are working on. - Yeah, that, I mean I think that's one of the things we\\nheard when we were out west is that this, this changes the way anyone\\ncan interact with a computer. Whether it's someone who wants to write a\\ntechnical piece of code or someone who just kind\\nof wants to take an idea and turn it into something that\\ncan be deployed technically. - I mean the interesting,\\nSource: <https://www.youtube.com/watch?v=Zo5M1lrMU1I&t=0s>\\n\\nContent: what people are working on. - Yeah, that, I mean I think that's one of the things we\\nheard when we were out west is that this, this changes the way anyone\\ncan interact with a computer. Whether it's someone who wants to write a\\ntechnical piece of code or someone who just kind\\nof wants to take an idea and turn it into something that\\ncan be deployed technically. - I mean the interesting,\\nand I'm really enjoying this, I talk about my, you know, back to the neural network\\nwith Professor Lerner was that you know, so much of the work historically went into training, you\\nknow, or into labeling data. Obviously the deep\\nlearning quickly allowed us to label, auto label data\\nand train these models. But what I didn't realize is\\nthis reinforcement learning with human feedback and different versions of that, you know, if you can get an expert\\nto react to the output of one of these models, you can immediately use that feedback and the model gets exponentially better, not just a little bit better. And so all of a sudden the user becomes the most valuable\\npiece of the equation. It's why I'm actually not\\nworried about the labor questions in many ways. I actually think people\\nare gonna find this remarkably democratic and democratizing 'cause they're gonna be able to do things they were never able to, and they in fact will add what they know that nobody else knew. And the model will get that\\nmuch better that quickly. But that's not the way we write software, and that's what, that's the work that's gotta be done. - Right. - Because you can't just say, okay, let's use it like an operating system. That isn't, like that's not works. - Interesting. We were talking a little bit earlier about how as products of\\nYale, you have a kind of a latent humanism that is\\nkind of baked into your brain regardless of what you end up doing. What, you know, as we\\nthink about strategically what makes the most sense for us to be doubling down on here, how do you see your own experience at Yale, and the work that you've been doing, and the things that you've been doing. - [Audience]  Yeah, yeah, yeah, I've been doing.\"}\n"
     ]
    }
   ],
   "source": [
    "print(llama_v2_chain({\"question\": \"What models use human instructions?\"}))\n",
    "\n",
    "print(llama_v2_chain({\"question\": \"Which are the advantage of each of these models?\"}))\n",
    "\n",
    "print(llama_v2_chain({\"question\": \"What are the downsides of your last model suggested above ?\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc0e3a-36df-4f2d-bc5b-b8a2263aa58f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########## With ConversationalRetrievalChain 1 ############\n",
    "doc_chain_custom_prompt=\"\"\"Given the following extracted parts of a long document and a question, create a final answer with \"SOURCES\" that represent exactly the Source name and link given.\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "{summaries}\n",
    "\n",
    "FINAL ANSWER:\"\"\"\n",
    "\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=doc_chain_custom_prompt, input_variables=[\"summaries\", \"question\"]\n",
    ")\n",
    "\n",
    "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "\n",
    "doc_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", prompt= qa_prompt, verbose=True)\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=vector_index.as_retriever(search_kwargs={'k': 6}),\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3191a48-aba5-42e8-b85f-afc886032ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######## just load document chain question ############\n",
    "doc_chain_custom_template=\"\"\"Given chunks from multiple documents and a question, create an answer to the question that references those documents as \"SOURCES.\n",
    "\n",
    "- If the question asks about the system's capabilities, the system should respond with some version of \"This system can answer questions about building AI-powered products across the stack, about large language models, and the Full Stack's courses and materials.\". The answer does not need to include sources.\n",
    "- If the answer cannot be determined from the chunks or from these instructions, the system should not answer the question. The system should instead return \"No relevant sources found\".\n",
    "- Chunks are taken from the middle of documents and may be truncated or missing context.\n",
    "- Documents are not guaranteed to be relevant to the question.\n",
    "- Always include the \"SOURCES\" in your answer otherwise it is not valid.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{sources}\n",
    "=========\n",
    "FINAL ANSWER:\"\"\"\n",
    "\n",
    "doc_chain_custom_prompt = PromptTemplate(\n",
    "    template=doc_chain_custom_template, input_variables=[\"sources\", \"question\"]\n",
    ")\n",
    "\n",
    "import prompts\n",
    "load_doc_chain_chain = load_qa_with_sources_chain(\n",
    "        llm,\n",
    "        chain_type=\"stuff\",\n",
    "        verbose=True,\n",
    "        prompt=doc_chain_custom_prompt, # prompts.main,\n",
    "        document_variable_name=\"sources\",\n",
    "    )\n",
    "\n",
    "\n",
    "def sim_que(query : str):\n",
    "    pretty_log(\"selecting sources by similarity to query\")\n",
    "    sources_and_scores = vector_index.similarity_search_with_score(query, k=3)\n",
    "\n",
    "    sources, scores = zip(*sources_and_scores)\n",
    "    #print(sources_and_scores)\n",
    "\n",
    "\n",
    "    result = load_doc_chain_chain(\n",
    "            {\"input_documents\": sources, \"question\": query}, return_only_outputs=True\n",
    "        )\n",
    "\n",
    "    answer = result[\"output_text\"]\n",
    "    print(answer)\n",
    "\n",
    "query1 = \"What models use human instructions?\"\n",
    "sim_que(query1)\n",
    "\n",
    "query2 = \"Are there any model trained on medical knowledge?\"\n",
    "sim_que(query2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2687e6-318f-4308-806f-6e37b1a49097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# doc_chain({\"question\":\"What models use human instructions?\", \"input_documents\":sources})\n",
    "\n",
    "chain({\"question\": \"What models use human instructions?\"})\n",
    "\n",
    "chain({\"question\": \"Which are the advantage of each of these models?\"})\n",
    "\n",
    "chain({\"question\": \"Can you elaborate more on point 3?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4b6c6-0e80-46f8-a0fa-f90cdf34cb4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain({\"question\":\"Which are the advantage of each of these models?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee460ef-2914-42bf-8aab-dda8955dce1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.qanda_llama2_cont()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ea16b-407a-4068-9ba1-7b194f17fef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "chat_history = \"\"\n",
    "input_list = [{\"user_input\": \"Give me some indications to solve a denial of service attack.\", \"chat_history\":chat_history}]\n",
    "\n",
    "start=time.time()\n",
    "test.llm_chain.generate(input_list)\n",
    "\n",
    "test.llm_chain.generate([{\"user_input\" : \"What question did I asked you previously\"}])\n",
    "\n",
    "end=time.time()\n",
    "\n",
    "print(f\"Total time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa58a52-4f28-428f-afbb-72efc29906dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "chat_history = \"\"\n",
    "input_list = [{\"user_input\": \"Give me some indications to solve a denial of service attack.\", \"chat_history\":chat_history}]\n",
    "\n",
    "start=time.time()\n",
    "answer1=test.llm_chain.predict(user_input=\"Give me some indications to solve a denial of service attack.\")\n",
    "print(answer1)\n",
    "answer2=test.llm_chain.predict(user_input=\"What question did I asked you previously\")\n",
    "print(answer2)\n",
    "end=time.time()\n",
    "\n",
    "print(f\"Total time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ba867-e64e-4b3c-bd20-5d0ca35f91c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#response = test.qanda_llama2(\"Can we combine LMMs and OCR?\", with_logging=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05bbb003be7165b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#response = test.qanda_llama2_withRAG(\"Can we combine LMMs and OCR?\", with_logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1b24d-2ff9-42b2-a46b-c0fc385943aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test.qanda_llama2_withRAG(\"Can we combine LLMs and OCR\", with_logging=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c76322-f71b-4272-9207-52c1faf46ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test.ask_question_withRAG(\"Can we combine LLMs and OCR\", with_logging=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a191cb-186f-4250-95ba-7f956df150ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema.runnable  import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable.utils import ConfigurableField\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4a6e4-f899-45d1-ad8d-d0511e7c9f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=test.embedding_engine)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt-llama\")\n",
    "llm=test.base_llm\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "########### \n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use 4 sentences maximum and keep the answer as concise as possible.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "rag_prompt_custom = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "#Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),        \n",
    "    }\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d6e2b-e96c-4fad-8342-333d9a10d164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_chain_with_source.invoke({question:\"What is Task Decomposition?\"})#\"Always say ```thanks for asking!``` at the end of the answer.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f252f-210a-4725-b735-7200f1b9e9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    prompt.invoke(\n",
    "        {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    "    ).to_string()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a354bc-c945-4304-a996-ff835b5c04b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, pipeline\n",
    "streamer = TextStreamer(test.tokenizer, skip_prompt=True)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=test.base_llm,\n",
    "    tokenizer=test.tokenizer,\n",
    "    max_length=2048,\n",
    "    temperature=0.6,\n",
    "    pad_token_id=test.tokenizer.eos_token_id,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    device=0,\n",
    "    streamer=streamer\n",
    ")\n",
    "pipe(prompts[0])\n",
    "\n",
    "inputs = test.tokenizer(prompts[0], return_tensors=\"pt\").to(device)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model_fintuned.generate(**inputs, streamer=streamer, pad_token_id=tokenizer.eos_token_id, max_length=248, temperature=0.8, top_p=0.8,\n",
    "                        repetition_penalty=1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0cc850-e0ec-4697-b4e9-15e56dea3666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
