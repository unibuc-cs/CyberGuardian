{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q66NZx7UFNff"
   },
   "source": [
    "## Using the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F0CkeXQnfMxq",
    "ExecuteTime": {
     "end_time": "2024-01-29T09:11:19.045440500Z",
     "start_time": "2024-01-29T09:11:13.381559100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline, TextStreamer\n",
    "import json\n",
    "import torch\n",
    "import textwrap\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "############# DEFINE SOME BASE PARSING AND PROMPTING WRAPPERS\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "load_in_8bit=QUANTIZATION_USE_8BIT = True\n",
    "load_in_4bit=QUANTIZATION_USE_4BIT = False\n",
    "USE_CACHE = False\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS if new_system_prompt is not None else \"\"\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "\n",
    "def generate(text):\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=512,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.eos_token_id,\n",
    "                                 )\n",
    "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
    "        final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs#, outputs\n",
    "\n",
    "def parse_print_text(text):\n",
    "        wrapped_text = textwrap.fill(text, width=100)\n",
    "        print(wrapped_text +'\\n\\n')\n",
    "        # return assistant_text\n",
    "    \n",
    "\n",
    "model_name = \"meta-llama/Llama-2-13b-chat-hf\" # \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          token=True,)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             token=True,\n",
    "                                            load_in_8bit=QUANTIZATION_USE_8BIT,\n",
    "                                            load_in_4bit=QUANTIZATION_USE_4BIT,\n",
    "                                             )\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens=4096,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                top_p=0.95,\n",
    "                min_length=None, #The minimum length of the sequence to be generated, input prompt + min_new_tokens,\n",
    "                repetition_penalty=1.0, #The parameter for repetition penalty. 1.0 means no penalty.\n",
    "                length_penalty=1, #[optional] Exponential penalty to the length that is used with beam-based generation.              \n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                streamer=streamer)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0.9})\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciprian/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! I'm just an AI, I don't have feelings or emotions, so I can't say how I am. However, I'm here to help answer any questions you may have to the best of my ability. How can I assist you today?</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"  Hello! I'm just an AI, I don't have feelings or emotions, so I can't say how I am. However, I'm here to help answer any questions you may have to the best of my ability. How can I assist you today?\""
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(get_prompt(\"Hello. How are you?\", \"\"))#DEFAULT_SYSTEM_PROMPT))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T14:21:49.201509700Z",
     "start_time": "2024-01-28T14:21:41.458932400Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "from statistics import mode\n",
    "\n",
    "def gen_answer():\n",
    "    response = llm(\n",
    "        \"John found that the average of 15 numbers is 40.\"\n",
    "        \"If 10 is added to each number then the mean of the numbers is?\"\n",
    "        \"Report the answer surrounded by three backticks, for example: ```123```\",\n",
    "        model = DEFAULT_MODEL, #meta-llama/Meta-Llama-3-70B-Instruct\n",
    "    )\n",
    "    match = re.search(r'```(\\d+)```', response)\n",
    "    if match is None:\n",
    "        return None\n",
    "    return match.group(1)\n",
    "\n",
    "answers = [gen_answer() for i in range(5)]\n",
    "\n",
    "print(\n",
    "    f\"Answers: {answers}\\n\",\n",
    "    f\"Final answer: {mode(answers)}\",\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import re\n",
    "from statistics import mode\n",
    "\n",
    "def gen_answer():\n",
    "    response = completion(\n",
    "        \"John found that the average of 15 numbers is 40.\"\n",
    "        \"If 10 is added to each number then the mean of the numbers is?\"\n",
    "        \"Report the answer surrounded by three backticks, for example: ```123```\",\n",
    "        model = DEFAULT_MODEL, #LLAMA2_70B_CHAT\n",
    "    )\n",
    "    match = re.search(r'```(\\d+)```', response)\n",
    "    if match is None:\n",
    "        return None\n",
    "    return match.group(1)\n",
    "\n",
    "answers = [gen_answer() for i in range(5)]\n",
    "\n",
    "print(\n",
    "    f\"Answers: {answers}\\n\",\n",
    "    f\"Final answer: {mode(answers)}\",\n",
    "    )\n",
    "\n",
    "complete_and_print(\n",
    "    \"\"\"\n",
    "    # Python code to calculate: ((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "    \"\"\",\n",
    "    model=\"meta/codellama-34b:67942fd0f55b66da802218a19a8f0e1d73095473674061a6ea19f2dc8c053152\"\n",
    ")\n",
    "\n",
    "complete_and_print(\"\"\"\n",
    "Calculate the answer to the following math problem:\n",
    "\n",
    "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "\"\"\")\n",
    "\n",
    "MENLO_PARK_TEMPS = {\n",
    "    \"2023-12-11\": \"52 degrees Fahrenheit\",\n",
    "    \"2023-12-12\": \"51 degrees Fahrenheit\",\n",
    "    \"2023-12-13\": \"51 degrees Fahrenheit\",\n",
    "}\n",
    "\n",
    "\n",
    "def prompt_with_rag(retrived_info, question):\n",
    "    complete_and_print(\n",
    "        f\"Given the following information: '{retrived_info}', respond to: '{question}'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def ask_for_temperature(day):\n",
    "    temp_on_day = MENLO_PARK_TEMPS.get(day) or \"unknown temperature\"\n",
    "    prompt_with_rag(\n",
    "        f\"The temperature in Menlo Park was {temp_on_day} on {day}'\",  # Retrieved fact\n",
    "        f\"What is the temperature in Menlo Park on {day}?\",  # User question\n",
    "    )\n",
    "\n",
    "\n",
    "ask_for_temperature(\"2023-12-12\")\n",
    "# \"Sure! The temperature in Menlo Park on 2023-12-12 was 51 degrees Fahrenheit.\"\n",
    "\n",
    "ask_for_temperature(\"2023-07-18\")\n",
    "\n",
    "\n",
    "### Limiting Extraneous Tokens\n",
    "\n",
    "A common struggle is getting output without extraneous tokens (ex. \"Sure! Here's more information on...\").\n",
    "\n",
    "Check out this improvement that combines a role, rules and restrictions, explicit instructions, and an example:\n",
    "\n",
    "\n",
    "complete_and_print(\n",
    "    \"Give me the zip code for Menlo Park in JSON format with the field 'zip_code'\",\n",
    "    model = LLAMA2_70B_CHAT,\n",
    ")\n",
    "# Likely returns the JSON and also \"Sure! Here's the JSON...\"\n",
    "\n",
    "complete_and_print(\n",
    "    \"\"\"\n",
    "    You are a robot that only outputs JSON.\n",
    "    You reply in JSON format with the field 'zip_code'.\n",
    "    Example question: What is the zip code of the Empire State Building? Example answer: {'zip_code': 10118}\n",
    "    Now here is my question: What is the zip code of Menlo Park?\n",
    "    \"\"\",\n",
    "    model = LLAMA2_70B_CHAT,\n",
    ")\n",
    "# \"{'zip_code': 94025}\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "######## REWRITE STUFF #############\n",
    "customRewritePrompt_System = (\"I ask you to rewrite the given text keeping the same meaning but with your own workds, do not copy paste sequence of words from the source. Replace the First-Person Pronouns in source with Third-person Pronouns. Do not use words like we, We, Our, and our.\")\n",
    "\n",
    "questionRewritePrompt = \"Rewrite me the following text {text}\"\n",
    "rewrite_prompt_template = get_prompt(questionRewritePrompt, new_system_prompt=customRewritePrompt_System)\n",
    "print(f\"Current rewrite prompt: {rewrite_prompt_template}\")\n",
    "rewrite_prompt = PromptTemplate(template=rewrite_prompt_template, input_variables=[\"text\"])\n",
    "rewrite_chain = LLMChain(prompt=rewrite_prompt, llm=llm)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "######### Summarize A TEXT ###########\n",
    "summarizationPrompt_User_template= \"Summarize the following article for me {text} with no more that {max_sentences} sentences. Replace the First-Person Pronouns in source with Third-person Pronouns. Do not use words like we or our.\"\n",
    "summarizationPrompt_System_template  = \"You are an expert on summarization and expressing key ideas succinctly. You speak as a top computer science professor. Do not copy paste sentences from the source.\"\n",
    "summarizationPrompt_template = get_prompt(summarizationPrompt_User_template, new_system_prompt=summarizationPrompt_System_template)\n",
    "print(f\"Current summarization prompt: {summarizationPrompt_template}\")\n",
    "summarizationPrompt = PromptTemplate(template=summarizationPrompt_template, input_variables=[\"text\", \"max_sentences\"])\n",
    "summarizationChain = LLMChain(prompt=summarizationPrompt, llm=llm)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def rewrite_text(text: str) -> str:\n",
    "    res = rewrite_chain.run({\"text\":text})\n",
    "    return res \n",
    "\n",
    "def summarize_text(text: str, max_sentences : int, num_runs) -> str:\n",
    "    for i in range(num_runs):\n",
    "        text = summarizationChain.run({\"text\":text, \"max_sentences\":max_sentences})\n",
    "    return text \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "IoT devices are increasingly being implicated in cyber-attacks, raising community concern about the risks they pose to critical infrastructure, corporations, and citizens. In order to reduce this risk, the IETF is pushing IoT vendors to develop formal specifications of the intended purpose of their IoT devices, in the form of a Manufacturer Usage Description (MUD), so that their network behavior in any operating environment can be locked down and verified rigorously. This article aims to assist IoT manufacturers in developing and verifying MUD profiles, while also helping adopters of these devices to ensure they are compatible with their organizational policies and track device network behavior using their MUD profile. Our first contribution is to develop a tool that takes the traffic trace of an arbitrary IoT device as input and automatically generates the MUD profile for it. We contribute our tool as open source, apply it to 28 consumer IoT devices, and highlight insights and challenges encountered in the process. Our second contribution is to apply a formal semantic framework that not only validates a given MUD profile for consistency, but also checks its compatibility with a given organizational policy. We apply our framework to representative organizations and selected devices, to demonstrate how MUD can reduce the effort needed for IoT acceptance testing. Finally, we show how operators can dynamically identify IoT devices using known MUD profiles and monitor their behavioral changes in their network.\"\"\"\n",
    "res = rewrite_text(text)\n",
    "#parse_print_text(res)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "In this project, the NCCoE demonstrated the ability to ensure that when an IoT device connects to a home or small-business network, MUD can automatically permit the device to send and receive only the traffic it requires to perform its intended function. The NCCoE has produced a practice guide to demonstrate the practicality and effectiveness of using the Internet Engineering Task Force’s Manufacturer Usage Description (MUD) standard to strengthen security for IoT devices on home and small-business networks.\n",
    "\"\"\"\n",
    "\n",
    "text = summarize_text(text, max_sentences=3, num_runs=1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "from etl.pdfs import concatenate_content_multiple_pages, extract_pdf\n",
    "\n",
    "documents = extract_pdf({'singlepdf': \"../data/special_smart_home/IoT_Dataset___Functional_Testing_Framework.pdf\"})\n",
    "res = concatenate_content_multiple_pages(documents)\n",
    "text = summarize_text(res, max_sentences=5, num_runs=1)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
