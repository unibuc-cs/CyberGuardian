{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " ## This implements the DEMO.png setup in this folder\n",
    " "
   ],
   "id": "a66955cc7c14cbf2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:41:35.231243Z",
     "start_time": "2024-07-02T11:41:33.818796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load a Llama-3-8B instruct \n",
    "import transformers\n",
    "import sys\n",
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from time import time\n",
    "#import chromadb\n",
    "#from chromadb.config import Settings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "import torch\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from typing import Union, List, Dict, Any, NoReturn\n",
    "from advancedRagUtils import convert_score_to_float\n",
    "\n"
   ],
   "id": "5f640589a6af7916",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ee61a3db697c43ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:42:03.126710Z",
     "start_time": "2024-07-02T11:41:35.232052Z"
    }
   },
   "source": [
    "# Test the AdaptiveRAGSupport\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from AdvancedRAG.AdaptiveRAGSupport import AdaptiveRAGSupport, logger\n",
    "from pprint import pprint\n",
    "model = None\n",
    "tokenizer = None\n",
    "retriever = None\n",
    "support = AdaptiveRAGSupport(model, tokenizer, retriever)\n",
    "\n",
    "question = \"agent memory\" #\"What is the best way to train a model?\"\n",
    "docs_content = support.get_docs_content_by_query(question, support.DEFAULT_MAX_DOCS_RETURNED, format_as_single_text=True, verbose=False)\n",
    "answer = support.answer_with_rag(question, docs_content, skip_prompt=True)\n",
    "logger.warning(answer)\n",
    "logger.warning(f\"answer is good for the question? {support.answer_grader(question, answer, skip_prompt=True)}\")\n",
    "logger.warning(f\"halucination grader, i.e., is the answer sustained by documents? {support.hallucination_grader(answer, docs_content, skip_prompt=True)}\")\n",
    "logger.warning(f\"question category: {support.question_router(question, skip_prompt=True)})\")\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10c9fd5d90e84d3ab782c672c740a759"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare model, tokenizer: 20.536 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The context discusses the concept of generative agents, which combines LLM with memory, planning, and reflection mechanisms to enable agents to behave conditioned on past experience. In this context, memory is used to record a comprehensive list of agents' experience in natural language, allowing the agent to reflect on past events and guide its future behavior.\n",
      "answer is good for the question? {'score': 'yes'}\n",
      "halucination grader, i.e., is the answer sustained by documents? {'score': 'yes'}\n",
      "question category: {'datasource': 'vectorstore'})\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "b78aa16cb3bc1878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:42:03.552454Z",
     "start_time": "2024-07-02T11:42:03.127820Z"
    }
   },
   "source": [
    "logger.warning(f\"question category: {support.question_router('What are the types of agent memory?', skip_prompt=True)})\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "question category: {'datasource': 'vectorstore'})\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "600ac2fcd5e09b10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:45:50.388982Z",
     "start_time": "2024-07-02T11:45:50.370333Z"
    }
   },
   "source": [
    "\n",
    "### Search\n",
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "0a7a98dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:45:55.011223Z",
     "start_time": "2024-07-02T11:45:54.987938Z"
    }
   },
   "source": [
    "from typing import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "class AgentMemory(TypedDict):\n",
    "    \"\"\"\n",
    "    Agent Memory to store the state of the agent and the conversation history \n",
    "    \"\"\"\n",
    "    \n",
    "    question: str # The last question asked by user\n",
    "    generation: str  # The last generated prompt from assistant\n",
    "    web_search: bool # Should do web search ?\n",
    "    documents: List[str] # The list of documents extracted\n",
    "    \n",
    "    \n",
    "    \n",
    "def retrieve(state: AgentMemory):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "    Args:\n",
    "        state: AgentMemory\n",
    "    Returns:\n",
    "        state (dict): New keys to be added to the state/update the state, containing the documents\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    logger.warning(f\"Retrieving documents for question: {question}\")\n",
    "    \n",
    "    documents = support.get_docs_content_by_query(question, support.DEFAULT_MAX_DOCS_RETURNED, format_as_single_text=False, verbose=False)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "    \n",
    "def generate(state: AgentMemory):\n",
    "    \"\"\"\n",
    "    Generate an answer using RAG model\n",
    "    Args:\n",
    "        state: AgentMemory\n",
    "    Returns:\n",
    "        state (dict): New keys to be added to the state/update the state, containing the generation\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    logger.warning(f\"Generating answer for question: {question}\")\n",
    "    \n",
    "    answer = support.answer_with_rag(question, documents, skip_prompt=True)\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": answer}\n",
    "\n",
    "def grade_documents(state: AgentMemory):\n",
    "    \"\"\"\n",
    "    Checks if the response is sustained by the documents. If any doc is not relevant, we set a flag to run a web search\n",
    "    Args:\n",
    "        state: AgentMemory\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updates the web_search flag\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    logger.warning(f\"Grading documents for question: {question}\")\n",
    "    \n",
    "    # Check if the answer is sustained by the documents\n",
    "    # Score each individually\n",
    "    filtered_docs = []\n",
    "    for doc in documents:\n",
    "        score = support.hallucination_grader(generation, doc.page_content, skip_prompt=True)\n",
    "        \n",
    "        grade = convert_score_to_float(score)\n",
    "        \n",
    "        # Document is relevant if the grade is above 0.5\n",
    "        if grade > 0.5:\n",
    "            filtered_docs.append(doc)\n",
    "        # If not, we set a flag to run a web search\n",
    "        else: \n",
    "            web_search = True\n",
    "            continue # IS THIS NEEDED ?????\n",
    "        \n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "def web_search(state: AgentMemory):\n",
    "    \"\"\"\n",
    "    Run a web search based on the question\n",
    "    \n",
    "    Args:\n",
    "        state: AgentMemory\n",
    "    Returns:\n",
    "        state (dict): Added the web search results to the state\n",
    "        \n",
    "    \"\"\"\n",
    "    # We run a web search\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Web search \n",
    "    logger.warning(f\"Running web search for question: {question}\")\n",
    "    web_search_docs = web_search_tool.invoke({\"query\": question, 'k': support.NUM_WEB_SEARCH_RESULTS})\n",
    "    web_results = \"\\n\".join(doc[\"content\"] for doc in web_search_docs)\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:    \n",
    "        documents = [web_results]\n",
    "        \n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "# Conditional edges\n",
    "def route_question(state: AgentMemory):\n",
    "    \"\"\"\n",
    "    Route the question to the right category, RAG or websearch\n",
    "    Args:\n",
    "        state: AgentMemory\n",
    "    Returns:\n",
    "        str: name of the next node to be executed\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    \n",
    "    logger.warning(f\"Routing question: {question}\")\n",
    "    \n",
    "    source = support.question_router(question, skip_prompt=True)\n",
    "    \n",
    "    logger.warning(f\"Question category: {source['datasource']}\")\n",
    "    if source[\"datasource\"] == \"websearch\":\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        return \"vectorstore\"\n",
    "    \n",
    "    \n",
    "    category = support.question_router(question, generation, documents, skip_prompt=True)\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation, \"category\": category}\n",
    "\n",
    "def decide_if_websearch_is_needed(state: AgentMemory):\n",
    "    \"\"\"\n",
    "    Check if we need to run the web search after a collection of documents were retrieved based on the state of the agent\n",
    "    Args:\n",
    "        state: AgentMemory\n",
    "    Returns:\n",
    "        str: name of the next node to be executed\n",
    "    \"\"\"\n",
    "    web_search = state[\"web_search\"]\n",
    "    logger.warning(f\"Assessing the status of web search: {web_search}\")\n",
    "    assert type(web_search) == bool, \"web_search should be a boolean\"\n",
    "    if web_search:\n",
    "        logger.warning(f\"---DECISION: Not all documents are relevant, running web search\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        logger.warning(f\"---DECISION: ALL documents are relevant, go to generation\")\n",
    "        return \"vectorstore\"\n",
    "    \n",
    "def grade_generation_vs_documents_and_question(state: AgentMemory):\n",
    "    \"\"\"\n",
    "    Determines if the generation is actually grounded in the documents retrieved from the vectorstore according and answering the question\n",
    "    Args:\n",
    "        state: AgentMemory\n",
    "    Returns:\n",
    "        str: Decision for next node to call.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    \n",
    "    logger.warning(f\"Hallucination check and then if the answer is useful: Grading generation vs documents and question\")\n",
    "    \n",
    "    # Check if answer is sustained by the documents, hallucination check\n",
    "    score_hallucination = support.hallucination_grader(question, documents, skip_prompt=True)\n",
    "    grade_hallucination = convert_score_to_float(score_hallucination)\n",
    "    if grade_hallucination > 0.5:\n",
    "        # Answer is sustained by the documents, now check if the answer is useful\n",
    "        logger.warning(f\"---DECISION: Generation is grounded in the documents\")\n",
    "                    \n",
    "        score_useful = support.answer_grader(question, generation, skip_prompt=True)\n",
    "        grade_useful = convert_score_to_float(score_useful)\n",
    "        if grade_useful > 0.5:\n",
    "            logger.warning(f\"---DECISION: Generation is sustained by the documents and the answer is useful\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            logger.warning(f\"---DECISION: Generation does not address question or is not useful\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        logger.warning(f\"---DECISION: Generation is NOT grounded in the documents, RETRYING\")\n",
    "        return \"not supported\"\n",
    "    \n",
    "workflow = StateGraph(AgentMemory)\n",
    "\n",
    "workflow.add_node(\"websearch\", web_search) # web search node\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve node\n",
    "workflow.add_node(\"generate\", generate) # generate node\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents node\n",
    "\n",
    "# workflow.add_node(\"route_question\", route_question) # route question node\n",
    "# workflow.add_node(\"decide_if_websearch_is_needed\", decide_if_websearch_is_needed) # decide if websearch is needed node\n",
    "# workflow.add_node(\"grade_generation_vs_documents_and_question\", grade_generation_vs_documents_and_question) # grade generation vs documents and question node\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "fa231f23ef7b2e60",
   "metadata": {},
   "source": [
    "## Building the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7dc59266bc98d55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:17:47.976075Z",
     "start_time": "2024-07-02T12:17:47.957048Z"
    }
   },
   "source": [
    "# Check the result \n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\" : \"websearch\",\n",
    "        \"vectorstore\" : \"retrieve\",\n",
    "    },\n",
    ") # Dict from result of the function to next node\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_if_websearch_is_needed,\n",
    "    {\n",
    "        \"websearch\" : \"websearch\",\n",
    "        \"generate\" : \"generate\", \n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_vs_documents_and_question,\n",
    "    {\n",
    "      \"not supported\" : \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "61972002d557530c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:17:55.322152Z",
     "start_time": "2024-07-02T12:17:49.897970Z"
    }
   },
   "source": [
    "from pprint import pprint\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test \n",
    "\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Routing question: What are the types of agent memory?\n",
      "Question category: vectorstore\n",
      "Retrieving documents for question: What are the types of agent memory?\n",
      "Grading documents for question: What are the types of agent memory?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Finished running: retrieve:'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assessing the status of web search: True\n",
      "---DECISION: Not all documents are relevant, running web search\n",
      "Running web search for question: What are the types of agent memory?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Finished running: grade_documents:'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answer for question: What are the types of agent memory?\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Finished running: websearch:'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hallucination check and then if the answer is useful: Grading generation vs documents and question\n",
      "---DECISION: Generation is grounded in the documents\n",
      "---DECISION: Generation is sustained by the documents and the answer is useful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Finished running: generate:'\n",
      "('According to the provided context, there are two categories of agent memory: '\n",
      " 'short-term and long-term memories.')\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6263bf30dbe191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
